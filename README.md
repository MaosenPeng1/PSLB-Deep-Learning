# Prpensity Score Local Balance (PSLB) Estimation Using Deep Learning
## Description
This project guides users how to implemented the PSLB deep learning method (LBC-Net) to estimate propensity scores introduced in 'A Deep Learning Approach to Nonparametric Propensity Score Estimation with Optimized Covariate Balance'. It consists of total 6 simulation settings and a real data analysis. All related files for each setting are contained in the zip file listed below,
- Kang and Schafer (2007) 
  - 5k (5000 sample size) [*]
    - True (correctly specified propensity score model)
    - Mis (mis-specified propensity score model)
    - Sensitivity Analysis (Network Layers)
  - 1k (1000 sample size)
    - True (correctly specified propensity score model)
    - Mis (mis-specified propensity score model)
- SSMR 
  - True (correctly specified propensity score model)
  - Mis (mis-specified propensity score model)
- EQLS (real data) 
  - Data Application [*]
  - Bootstrap (standard deviation)

*: The results of the file are shown in the main context of the article. 

Each file consists of two parts, estimation and evaluation. The estimation related to the deep learning are conducted using Python (.py) and others are implemented with R (.R). Besides the PSLB method, the file also includes logistic regression, CBPS, and deep learning method with BCE loss for estimation comparison. The figures and tables in the article are contained in the evaluation files. A seed file is in each simulation files so that the results can be reproductive. The details will illustrated below, along with a step by step guide using Kang and Schafer (2007) of 5000 sample size as an example.

## Installation and Setup
### Codes and Resources Used
* R: version -- 4.3.1
Editor: RStudio
Packages Required:
CBPS, dplyr, tidyr, ggplot2, knitr, kableExtra.

* Python: version -- 3.11.3
Editor: Visual Studio Code
Packages Required:
torch, numpy, pandas, sys, argparse.

* Sever: High Performance Computing (HPC) - Seadragon Cluster (Linux system)
Editor: X-Win32
Hardware Resources: Central Processing Unit (CPU)
Purpose: run deep learning code parallel on sever. The bash file (.lsf) is generated by the file called 'lsf_generation*.py'.

## Data
The simulated data is generated using the "*Simulation.R" file for all seeds in 'sim_seed.csv'. 

The real data here is the European Quality of Life Survey (EQLS) study data, which is available from the UK Data Service https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=7724\#!/access-data. The 'Get_Data_EQLS.R' presents the data cleaning details and descriptive analysis of the data. 
Here we also uploaded the cleaned data in file `eqls_data.csv` as long as the file illustrating how we clean the data `Get_Data_EQLS.R`. The EQLS is a survey conducted through questionnaires that encompasses adults from 35 European countries. We merged the data from the 2007 and 2011 iterations of the EQLS. Our focus is on determining whether conflicts between work and life balance, as self-reported by respondents, influence the mental well-being of the working adult population. The 2007 EQLS included 35,635 individuals, while the 2011 version had 43,636 participants. For ease of analysis, we omitted 21,800 participants from the 2007 dataset and 27,234 from the 2011 dataset due to incomplete information. Our final sample is comprised of 17,439 individuals experiencing conflicts in balancing work and life (either at work, at home, or both, referred to as cases) and 12,797 individuals with no or minor conflicts (referred to as controls). We assessed mental well-being as the outcome using the World Health Organization Five (WHO-5) well-being index, which ranges from 0 to 100. This index evaluates the respondent's emotional state over the preceding two weeks. The number of covariates is 70.

## Instructions
1. generate data using "*Simulation.R", you will get data and ck_h csv files. The ck_h contains the local points and adaptive bandwidth by default, one can specify or define there own local intervals. Note all R functions are included in the "Method.R" file.
2. implement different methods to the data to estimate the propensity scores. Logistic regression and CBPS methods can be applied by "Logistic and CBPS Model.R", which will output a R object saved to the local computer for further use. The BCE loss and PSLB-DL method are run on the sever using the "\*.py" files through the bash file generated from "lsf_generation*.py".
3. evaluate the global balance, local balance, and outcome estimation (mean outcome or average treatment effect) with "Figures and Tables.R". This code can output figures and tables presented in the article. 

## Step-by-step Example (Kang and Schafer (2007).zip -> 5k (5000 sample size))
To facilitate the review process and ensure the reproducibility of our results, please follow the detailed instructions below, organized into distinct steps for clarity and ease of execution. These steps are designed to guide you through the simulation, analysis, and evaluation phases of our study using the provided scripts and data files.

1. Data Simulation
- Script: `Kang_Schafer_Simulation.R`
- Seed File: `sim_seed.csv`
- Output: 100 simulated datasets and corresponding `ck_h.csv` files.
- Parameters: Default span $\rho=0.1$ is used to determine the locality and compute the adaptive bandwidth.
- Execution Environment: High-Performance Computing (HPC) cluster.
- Command: After loading the R module (`module load R`), execute the simulation script using `Rscript Kang_Schafer_Simulation.R`.
- Purpose: This step generates simulated data essential for the subsequent analysis.

2. Bash Files Generation and Execution
Generate bash files for different analytical methods using the following Python scripts:
- Logistic and CBPS Method: `parallel_logistic_cbps_lsf_generation.py`
  - Requirements: 40 cores for parallel processes and total 200 GB memory.
- LBC-Net Method: `PSLB_lsf_generation.py`
  - Requirements: At least 2GB memory, 100 CPUs with 1 core, and 2 hours of wall-time; CPUs suffice for execution, although GPU nodes can be utilized if available (available gpu resources in our study is `cuda10.0/toolkit/10.0.130`).
- BCE Method: `BCE_lsf_generation.py`
  - Requirements: Similar to LBC-Net method. 
- Learning Rate Optimization: `Learning_rate_lsf_generation.py`
  - Purpose: Conduct a grid search for the learning rate for both BCE and LBC-Net methods. Evaluate tuning parameters by plotting a scatter plot of loss $Q(\theta)$ to observe convergence behavior.

Follow the embedded instructions within each script to run the corresponding programs on the cluster, either in Python or R. After execution, download all resulting CSV files to your local machine for analysis.

3. Analysis and Evaluation
- Script for Figures and Tables: `Figures and Tables.R`
  - This script utilizes functions defined in `Methods.R` to reproduce all figures and tables presented in the article.
- Purpose: To evaluate the results and verify the reproducibility of our findings.

### Additional Steps for Sensitivity Analysis
To assess the robustness of our findings, we conducted a sensitivity analysis focusing on different network structures (ranging from 3 to 6 layers). This analysis is detailed in the `Sensitivity Analysis (Network Layers)` file, and the steps for execution are similar to those outlined above.

### Training Details and Hyperparameter Selection
Data Preparation:
- Data Loading: Data and bandwidths were loaded using a specific seed to ensure reproducibility. 
- Normalization and Data Processing: A custom dataloader was employed to normalize the data and add an intercept term, preparing the data for model training.

Model Configuration:
- Initial Parameters:
  - Batch Size: Determined based on dataset size (batch gradient descent).
  - Input Dimensions: Set according to the number of covariates in the dataset.
  - Hidden Dimensions: Chosen from experience, balancing between the number of covariates and the total sample size to optimize model complexity and prevent overfitting.
  - Output Dimensions: Configured to match the requirements of the propensity score estimation.
  - Learning Rate: Initially selected through preliminary experiments, then fine-tuned using a grid search approach to identify the optimal value that minimizes the loss function.
  - Total Epochs: Set to ensure sufficient model training ($20000$).

Model Architecture:
- Network Structure: The model is a three-layer fully connected neural network, incorporating batch normalization and residual connections to enhance learning dynamics and stability.
- VAE for Initial Weights: A Variational Autoencoder (VAE) model was implemented to generate initial weights for the propensity score model, facilitating a more effective learning start point.
- Customized Loss Functions:
  - local_balance_ipw_loss: $Q1$ in the article.
  - penalty_loss (referred to as calibration loss in the article): $Q2$ in the article.

Please ensure that each step is followed carefully to guarantee the accurate reproduction of our studyâ€™s results. Your attention to detail and adherence to these instructions are greatly appreciated and essential for a thorough review process.

## Real Data Analysis
Computational Resources and Configuration:
- Wall-Time: Given the complexity and size of the real dataset, we allocated 6 hours of wall-time for all methods.
- Memory Allocation: 6GB of memory is reserved for CPU nodes
- GPU Utilization for LBC-Net Method: a GPU node equipped with 16 cores (`cuda10.0/toolkit/10.0.130`). 

The processing steps for the real data analysis mirror those described in the simulation study section. This consistency ensures that the methodologies are applied systematically across different datasets, allowing for direct comparisons of performance and outcomes. Additionally, `Bootstrap (standard deviation)` file contains scripts implementing 100 bootstrap simulations to assess the stability (bootstrap standard deviation) of all methods when applied to real-world data. Each simulation replicates the data analysis process, applying our methods to resampled datasets. 

## LICENSE
This project is licensed under the MIT License - see the LICENSE.txt file for details.

## Reference
Kang, J. D. and Schafer, J. L. Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. Statistical Science, 22(4):523â€“539, 2007








  
